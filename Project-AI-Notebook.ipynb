{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9385f23",
   "metadata": {},
   "source": [
    "# CMPS285-AI-Project\n",
    "Done by:Jad Raad, Ali Younes, Ali Hamdan, Ahmad Termos\n",
    "Presented to: Dr. Ahmad Elhaj\n",
    "Fall 2023-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce372c5",
   "metadata": {},
   "source": [
    "# This part of the code is responsible for creating a simple Tkinter-based graphical user interface (GUI) for a plagiarism checker application.\n",
    "\n",
    "Let's go through each component and method in detail:\n",
    "\n",
    "Import Statements:\n",
    "\n",
    "import tkinter as tk: Imports the Tkinter module, which provides a set of tools for creating graphical user interfaces.\n",
    "from tkinter import filedialog: Imports the filedialog submodule from Tkinter, which provides functions for opening and saving files.\n",
    "import requests: Imports the requests module, which is used for making HTTP requests to the FastAPI backend.\n",
    "\n",
    "Class Definition: PlagiarismCheckerApp\n",
    "\n",
    "__init__ method:\n",
    "\n",
    "Initializes the main application window (root) and sets its title to \"Plagiarism Checker\".\n",
    "Creates various UI elements using Tkinter widgets such as buttons and labels.\n",
    "Sets up the layout by packing these widgets with specified padding.\n",
    "Initializes the file_path attribute to None, which will be used to store the path of the uploaded document.\n",
    "\n",
    "upload_file method:\n",
    "\n",
    "Invokes the file dialog to allow the user to select a file for upload.\n",
    "If a file is selected, the file path is stored in the file_path attribute. \n",
    "check_plagiarism method:\n",
    "\n",
    "Checks if a file has been uploaded. If not, it updates the result label with a message indicating that a document needs to be uploaded.\n",
    "If a file is uploaded, it sends a POST request to the FastAPI backend (http://127.0.0.1:8000/detect) with the file content.\n",
    "Parses the response and updates the result label with the plagiarism detection result.\n",
    "\n",
    "reset method:\n",
    "\n",
    "Resets the state of the application, allowing the user to upload a new file. It resets the result label and sets the file_path attribute to None.\n",
    "\n",
    "Main Execution Block:\n",
    "Checks if the script is being run as the main program (if __name__ == \"__main__\":).\n",
    "Creates a Tkinter root window (root), and an instance of the PlagiarismCheckerApp class (app) is created with this root window.\n",
    "The mainloop() method is called on the root window, which starts the Tkinter event loop, allowing the user to interact with the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import requests\n",
    "\n",
    "class PlagiarismCheckerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Plagiarism Checker\")\n",
    "\n",
    "        # UI Elements\n",
    "        self.upload_button = tk.Button(root, text=\"Upload Document\", command=self.upload_file)\n",
    "        self.check_button = tk.Button(root, text=\"Check Plagiarism\", command=self.check_plagiarism)\n",
    "        self.result_label = tk.Label(root, text=\"Plagiarism Result: \")\n",
    "        self.reset_button = tk.Button(root, text=\"Reset\", command=self.reset)\n",
    "\n",
    "        # Layout\n",
    "        self.upload_button.pack(pady=10)\n",
    "        self.check_button.pack(pady=10)\n",
    "        self.result_label.pack(pady=10)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Initialize file_path attribute\n",
    "        self.file_path = None\n",
    "\n",
    "    def upload_file(self):\n",
    "        file_path = filedialog.askopenfilename()\n",
    "        if file_path:\n",
    "            self.file_path = file_path\n",
    "            # You can handle the uploaded file here (e.g., display the file path)\n",
    "\n",
    "    def check_plagiarism(self):\n",
    "        if not self.file_path:\n",
    "            self.result_label.config(text=\"Upload a document first.\")\n",
    "            return\n",
    "\n",
    "        # Send the file path to the FastAPI backend for plagiarism detection\n",
    "        url = \"http://127.0.0.1:8000/detect\"\n",
    "        files = {'file': open(self.file_path, 'rb')}\n",
    "        data = {'file_path': self.file_path}\n",
    "        response = requests.post(url, files=files, data=data)\n",
    "\n",
    "        # Parse the response and update the result_label with the plagiarism result\n",
    "        if response.status_code == 200:\n",
    "            similarity_percentage = float(response.json()['similarity_percentage'])\n",
    "            result_text = f\"Plagiarism Result: {similarity_percentage:.2f}%\"\n",
    "            self.result_label.config(text=result_text)\n",
    "        else:\n",
    "            self.result_label.config(text=\"Plagiarism check failed\")\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state to allow uploading a new file\n",
    "        self.result_label.config(text=\"Plagiarism Result: \")\n",
    "        self.file_path = None  # Reset the file path attribute\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = PlagiarismCheckerApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa76509",
   "metadata": {},
   "source": [
    "# This part of the code is responsible for the FastAPI backend of the plagiarism checker. \n",
    "\n",
    "It includes functions for loading a dataset, training a new plagiarism model, and handling the plagiarism detection endpoint.\n",
    "\n",
    "Let's break down each component:\n",
    "\n",
    "Import Statements:\n",
    "import pandas as pd: Imports the Pandas library for data manipulation and analysis.\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException: Imports necessary modules and classes from FastAPI for creating the API.\n",
    "from fastapi.responses import JSONResponse: Imports JSONResponse class from FastAPI for returning JSON responses.\n",
    "import shutil: Imports the shutil module for file operations.\n",
    "import os: Imports the os module for operating system-related functions.\n",
    "from PlagiarismChecker import train_plagiarism_model: Imports the train_plagiarism_model function from a module named PlagiarismChecker.\n",
    "\n",
    "FastAPI Setup:\n",
    "\n",
    "app = FastAPI(): Initializes a FastAPI application.\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\user\\\\Downloads\\\\CMPS285-AI-Project\\\\fake.xlsx': Specifies the path to the Excel dataset. You should replace this with the actual path to your dataset.\n",
    "\n",
    "Dataset Loading Function: load_dataset\n",
    "\n",
    "This function attempts to load the Excel dataset specified by dataset_path.\n",
    "If successful, it checks whether the required columns ('text' and 'label') are present in the dataset.\n",
    "If everything is in order, it extracts the 'text' and 'label' columns from the DataFrame and returns them as lists.\n",
    "If any error occurs during dataset loading, it prints an error message and returns empty lists.\n",
    "\n",
    "Model Training Function: train_new_model\n",
    "\n",
    "Calls the load_dataset function to get the 'text' and 'label' lists.\n",
    "Calls the train_plagiarism_model function from the imported module, passing the 'text' and 'label' lists.\n",
    "Returns the trained plagiarism model.\n",
    "\n",
    "Plagiarism Detection Endpoint: @app.post(\"/detect\")\n",
    "\n",
    "This is a FastAPI endpoint for handling POST requests to the \"/detect\" path.\n",
    "It receives an uploaded file (file) and a file path (file_path) as form data.\n",
    "Calls train_new_model to obtain a trained plagiarism model.\n",
    "Temporarily saves the uploaded file to \"temp_file.txt\" using shutil.copyfileobj.\n",
    "Tries to predict plagiarism using the trained model and a sample text. If an exception occurs during prediction, it returns an error response.\n",
    "Finally, removes the temporary file.\n",
    "Returns a JSON response containing the calculated similarity percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14871014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import shutil\n",
    "import os\n",
    "from PlagiarismChecker import train_plagiarism_model\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\user\\\\Downloads\\\\CMPS285-AI-Project\\\\fake.xlsx'  # Replace with your Excel dataset path\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    try:\n",
    "        # Load the Excel file into a DataFrame\n",
    "        df = pd.read_excel(dataset_path, engine='openpyxl')\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "\n",
    "        # Check if 'text' and 'label' columns are present\n",
    "        if 'text' not in df.columns or 'label' not in df.columns:\n",
    "            print(\"Columns 'text' and 'label' are required in the dataset.\")\n",
    "            return [], []\n",
    "\n",
    "        # Access columns using their names\n",
    "        texts = df['text'].tolist()\n",
    "        labels = df['label'].tolist()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{dataset_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The Excel file is empty.\")\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def train_new_model():\n",
    "    # Load the dataset for each new file\n",
    "    train_texts, train_labels = load_dataset(dataset_path)\n",
    "\n",
    "    # Train a new plagiarism model for each new file\n",
    "    return train_plagiarism_model(train_texts, train_labels)\n",
    "\n",
    "@app.post(\"/detect\")\n",
    "async def detect_plagiarism(file: UploadFile = File(...), file_path: str = Form(...)):\n",
    "    # Train a new model for each new file\n",
    "    plagiarism_model = train_new_model()\n",
    "\n",
    "    if plagiarism_model is None:\n",
    "        return JSONResponse(content={\"error\": \"Failed to load or train the plagiarism model.\"}, status_code=500)\n",
    "\n",
    "    # Save the uploaded file temporarily\n",
    "    with open(\"temp_file.txt\", \"wb\") as temp_file:\n",
    "        shutil.copyfileobj(file.file, temp_file)\n",
    "\n",
    "    try:\n",
    "        # Predict plagiarism using the trained model\n",
    "        similarity_percentage = plagiarism_model.predict([\"text to compare with the uploaded document\"])[0] * 100\n",
    "    except Exception as e:\n",
    "        # Handle any errors that might occur during prediction\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "    finally:\n",
    "        # Remove the temporary file\n",
    "        os.remove(\"temp_file.txt\")\n",
    "\n",
    "    # Return the similarity percentage in the response\n",
    "    return JSONResponse(content={\"similarity_percentage\": similarity_percentage})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa29bf",
   "metadata": {},
   "source": [
    "# This part of the code is responsible for Plagiarsim Checker Module:\n",
    "\n",
    "It starts by defining a dataset class (PlagiarismDataset), a plagiarism model class (PlagiarismModel), and a function (train_plagiarism_model) to train the plagiarism detection model using BERT (Bidirectional Encoder Representations from Transformers) from the Hugging Face Transformers library.\n",
    "\n",
    "Let's break down each component:\n",
    "\n",
    "PlagiarismDataset Class:\n",
    "\n",
    "This class inherits from the Dataset class in PyTorch and is responsible for preparing the data for training the plagiarism model.\n",
    "\n",
    "__init__ Method:\n",
    "Initializes the dataset with texts, labels, a tokenizer, and a maximum sequence length.\n",
    "\n",
    "__len__ Method:\n",
    "Returns the number of samples in the dataset.\n",
    "\n",
    "__getitem__ Method:\n",
    "Retrieves a specific sample at the given index (idx).\n",
    "Converts the text and label to the appropriate format for model input.\n",
    "Tokenizes the text using the provided tokenizer (BertTokenizer).\n",
    "Returns a dictionary containing 'input_ids', 'attention_mask', and 'label'.\n",
    "\n",
    "PlagiarismModel Class:\n",
    "\n",
    "This class defines the plagiarism detection model using BERT.\n",
    "\n",
    "__init__ Method:\n",
    "Initializes the model with a tokenizer, the BERT model, and a device (default is set to 'cuda' if GPU is available, otherwise 'cpu').\n",
    "\n",
    "train Method:\n",
    "Trains the model using the provided training texts and labels.\n",
    "Tokenizes the texts using the tokenizer and creates a DataLoader for training.\n",
    "Uses the AdamW optimizer for training.\n",
    "Iterates through the specified number of epochs, batches, and updates the model parameters based on the training loss.\n",
    "\n",
    "predict Method:\n",
    "Evaluates the model on a set of texts and returns the predicted probabilities for plagiarism.\n",
    "\n",
    "predict_with_similar_text Method:\n",
    "Evaluates the model on a set of texts and returns both the predicted probabilities for plagiarism and a list of similar texts.\n",
    "\n",
    "train_plagiarism_model Function:\n",
    "This function creates an instance of the PlagiarismModel class, loads the BERT tokenizer, and initializes the BERT model for sequence classification.\n",
    "\n",
    "Error Handling:\n",
    "Checks if there is any training data provided. If not, it prints an error message and returns None.\n",
    "\n",
    "Model Training:\n",
    "Calls the train method of the plagiarism model to train the model using the provided texts and labels.\n",
    "Return:\n",
    "\n",
    "Returns the trained plagiarism model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6f5c3",
   "metadata": {},
   "source": [
    "# BERT Tokenizer\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is employed as the underlying model for plagiarism detection. BERT is a powerful pre-trained natural language processing model developed by Google. It has been pre-trained on a large corpus of text data and has shown remarkable performance in various natural language understanding tasks.\n",
    "\n",
    "Let's break down the role of BERT in your project:\n",
    "\n",
    "Tokenizer (BertTokenizer):\n",
    "\n",
    "BERT operates on fixed-size sequences of tokens. The BertTokenizer is responsible for tokenizing the input text into subwords that BERT can understand.\n",
    "It also adds special tokens, such as [CLS] (classification) and [SEP] (separator), to the beginning and end of the tokenized sequence.\n",
    "Model Architecture (BertForSequenceClassification):\n",
    "\n",
    "The BertForSequenceClassification model is a variant of BERT fine-tuned for sequence classification tasks, such as sentiment analysis or, in your case, plagiarism detection.\n",
    "It consists of the BERT base model architecture with an additional classification layer on top.\n",
    "The classification layer is trained to predict the class labels (in your case, whether a given text is plagiarized or not) based on the representation learned by BERT.\n",
    "Training:\n",
    "\n",
    "The train_plagiarism_model function initializes an instance of PlagiarismModel, where BERT is used as the underlying model.\n",
    "The model is then trained using a dataset that contains labeled examples of text and their corresponding labels (plagiarized or not).\n",
    "During training, the model's weights are updated to minimize the classification loss, improving its ability to distinguish between plagiarized and non-plagiarized text.\n",
    "Prediction:\n",
    "\n",
    "The trained model can be used for plagiarism detection on new, unseen texts. The predict method takes a list of texts and returns the predicted probabilities of plagiarism for each text.\n",
    "The predict_with_similar_text method not only provides plagiarism probabilities but also identifies similar texts based on the predicted labels.\n",
    "Embedding Semantic Information:\n",
    "\n",
    "BERT captures rich semantic information and context from the input text. The pre-trained BERT model has learned contextualized embeddings for each token, allowing it to understand the relationships between words and their contexts.\n",
    "This contextualized information is crucial for understanding the nuances in natural language and is beneficial for tasks like plagiarism detection where the subtle differences in language usage are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ea918",
   "metadata": {},
   "source": [
    "# AdamW optimizer\n",
    "\n",
    "Initialization:\n",
    "self.model.parameters(): Retrieves the parameters (weights) of the plagiarism detection model.\n",
    "lr=learning_rate: Sets the learning rate, which controls the step size during optimization.\n",
    "\n",
    "Zeroing Gradients:\n",
    "Before computing the gradients, the optimizer is instructed to zero out the gradients of the model parameters. This is necessary to avoid accumulating gradients from previous iterations.\n",
    "\n",
    "Forward Pass:\n",
    "Performs a forward pass through the plagiarism detection model (self.model).\n",
    "Calculates the loss based on the model's predictions (outputs) and the ground truth labels (labels).\n",
    "\n",
    "Backward Pass:\n",
    "Performs a backward pass to compute the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "Weight Decay:\n",
    "Updates the model parameters using the calculated gradients and the specified learning rate.\n",
    "Additionally, AdamW applies weight decay during parameter updates. Weight decay penalizes large weights by subtracting a fraction of the weights from the gradient. This helps prevent overfitting by discouraging overly complex models.\n",
    "\n",
    "Repeating the Process:\n",
    "Steps 2-5 are repeated for each batch in the training data.\n",
    "\n",
    "Monitoring Training:\n",
    "Accumulates the loss for each batch to calculate the average loss later.\n",
    "\n",
    "Epoch Summary:\n",
    "Prints the average loss for the epoch, providing feedback on the training progress.\n",
    "\n",
    "\n",
    "the hyperparameters are set as follows:\n",
    "\n",
    "-epochs=3: The model is trained for three epochs. An epoch is one complete pass through the entire training dataset.\n",
    "-batch_size=16: Training is performed in batches, and each batch contains 16 samples. Batch training is a common technique to make the optimization process more computationally efficient and to leverage parallel processing capabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class PlagiarismDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        try:\n",
    "            label = int(self.labels[idx])\n",
    "        except ValueError:\n",
    "            # Handle the case where the label is not a valid integer\n",
    "            print(f\"Warning: Invalid label '{self.labels[idx]}' at index {idx}. Setting label to 0.\")\n",
    "            label = 0\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class PlagiarismModel:\n",
    "    def __init__(self, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, train_texts, train_labels, epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "        train_dataset = PlagiarismDataset(train_texts, train_labels, self.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        \n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "\n",
    "    def predict(self, texts):\n",
    "        self.model.eval()\n",
    "        encoded_texts = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = encoded_texts['input_ids'].to(self.device)\n",
    "        attention_mask = encoded_texts['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        return probabilities[:, 1].cpu().numpy()\n",
    "\n",
    "    def predict_with_similar_text(self, texts):\n",
    "            self.model.eval()\n",
    "            encoded_texts = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoded_texts['input_ids'].to(self.device)\n",
    "            attention_mask = encoded_texts['attention_mask'].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "\n",
    "            # Extract similar text\n",
    "            similar_texts = [text if label == 1 else \"\" for text, label in zip(texts, predicted_labels)]\n",
    "            return probabilities[:, 1].cpu().numpy(), similar_texts\n",
    "\n",
    "def train_plagiarism_model(train_texts, train_labels):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    plagiarism_model = PlagiarismModel(tokenizer, model)\n",
    "\n",
    "    if len(train_texts) == 0 or len(train_labels) == 0:\n",
    "        print(\"Error: No training data provided.\")\n",
    "        return None\n",
    "\n",
    "    plagiarism_model.train(train_texts, train_labels)\n",
    "\n",
    "    return plagiarism_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
